{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f601f3",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "For Colab, the helper folders need to be copied over from the repo. The below cell does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb2f61",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Check if the environment variable exists\n",
    "if [ -n \"$COLAB_RELEASE_TAG\" ] || [ -n \"$COLAB_GPU\" ]; then\n",
    "    echo \"Running on Google Colab. Cloning repository into temp folder...\"\n",
    "    git clone https://github.com/TuebingenAICenter/agent-tutorial.git /tmp/tmp_repo\n",
    "    echo \"Moving all helpers to project root...\"\n",
    "    mv /tmp/tmp_repo/chat_with_X_utils .\n",
    "    mv /tmp/tmp_repo/images .\n",
    "    mv /tmp/tmp_repo/env.example ./.env\n",
    "    mv /tmp/tmp_repo/requirements.txt .\n",
    "else\n",
    "    echo \"Not running on Google Colab. Skipping git clone.\"\n",
    "fi\n",
    "\n",
    "# The installation block runs regardless of environment.\n",
    "echo \"Checking for requirements.txt and installing required packages...\"\n",
    "\n",
    "# Check if requirements.txt exists in the current directory\n",
    "if [ -f \"requirements.txt\" ]; then\n",
    "    # Attempt to install with 'uv', and if it fails (exit code != 0), use 'pip' as a fallback.\n",
    "    if command -v uv &> /dev/null; then\n",
    "        echo \"uv detected. Installing with uv...\"\n",
    "        uv pip install -r requirements.txt\n",
    "    else\n",
    "        echo \"Installing with pip...\"\n",
    "        pip install -r requirements.txt\n",
    "    fi\n",
    "else\n",
    "    echo \"ERROR! requirements.txt not found! Please check for errors...\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2bd50",
   "metadata": {},
   "source": [
    "### Setting API key\n",
    "The following cell sets the API key for accessing LLMs. The prompt will ask for `OPENROUTER_API_KEY` if it has not been set in the .env file.\n",
    "\n",
    "Optionally an OpenAI key can be set in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cced317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Load environment variables from a .env file if it exists\n",
    "dotenv.load_dotenv()\n",
    "  \n",
    "# Prompt for the API key if it's not already set\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = getpass(\n",
    "        \"Enter your OPENROUTER API key: \"\n",
    "    )\n",
    "            \n",
    "if not os.environ[\"OPENROUTER_API_KEY\"]:\n",
    "    print(\"WARNING: API key not set. Please run this cell again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90481e",
   "metadata": {},
   "source": [
    "# Example 02: RAG-based YouTube/PDF Chat\n",
    "\n",
    "Now we know how a basic LangGraph system is implemented. Let's upgrade our tools towards a more useful application that let's us chat with an LLM about any PDF document or YouTube video.\n",
    "\n",
    "**What:** chatbot using more complex tools, some of which will require human oversight to use.\n",
    "\n",
    "**Why:** cement advanced concepts via implementation\n",
    "\n",
    "**Live:** this is the notebook called `..._scaffolded.ipynb` that has some crucial lines of code missing. Follow along and see if you can fill in those lines to get the example working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd09ed",
   "metadata": {},
   "source": [
    "## What we plan to create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e9682",
   "metadata": {},
   "source": [
    "<img src=\"./images/Agent_02_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7452",
   "metadata": {},
   "source": [
    "## What it looks like in LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce67a4",
   "metadata": {},
   "source": [
    "<img src=\"./images/Agent_02_conditional_edges_colored_in.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d5bb9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23984ad",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Optional, Literal\n",
    "from datetime import datetime\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import ToolNode, tools_condition, InjectedState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage,\n",
    ")\n",
    "\n",
    "# Local utils (explicit imports)\n",
    "from chat_with_X_utils.metadata_mangement import load_metadata\n",
    "from chat_with_X_utils.tool_utils import (\n",
    "    get_documents,\n",
    "    embed_documents,\n",
    "    parse_retrieval,\n",
    "    create_selection_summary,\n",
    "    get_database_info,\n",
    ")\n",
    "from chat_with_X_utils.print_utils import (\n",
    "    print_messages_from_stream_event as _print_messages_from_stream_event,\n",
    "    print_messages_from_state as _print_messages_from_state,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809f0d9",
   "metadata": {},
   "source": [
    "### Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDINGS_MODEL = \"text-embedding-3-small\"\n",
    "VECTOR_DB_PATH = \"./chroma_db\"\n",
    "METADATA_FILE = \"./document_metadata.json\"\n",
    "\n",
    "# Initialize components (single LLM)\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4.1-mini-2025-04-14\",\n",
    "        temperature=0.0,\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    )\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=EMBEDDINGS_MODEL, \n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    )\n",
    "else:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4.1-mini-2025-04-14\",\n",
    "        temperature=0.0,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "    )\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    # Use a local model for embeddings \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Initialize or load vector store\n",
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_DB_PATH,\n",
    "    collection_name=\"rag_documents\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74037c88",
   "metadata": {},
   "source": [
    "## Chat with PDFs and YouTube videos (How to use tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63437e",
   "metadata": {},
   "source": [
    "The following cells contain some missing code for you to complete. Places where you need to fill in the missing code are marked by comments like:\n",
    "\n",
    "```# TODO: Do XYZ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3575b4",
   "metadata": {},
   "source": [
    "### [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)\n",
    "\n",
    "Remember that the State ( an instance of `TypedDict`) is the main object that all nodes operate on & communicate with. In our current design, that state is defined to have the following keys:\n",
    "\n",
    "- `messages` stores all messages to and from our LLM. *Notice:* we need a reducer function to integrate the update to the state sent by nodes.\n",
    "\n",
    "- `documents_to_chat`: stores the document we want to chat with, that is used as an index for the embedded chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"Main state for the RAG system\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    documents_to_chat: List[str]  # Selected document keys (doc_keys)\n",
    "\n",
    "# TODO: Build the graph for this state. \n",
    "design_graph = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3b7f9",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60ca10",
   "metadata": {},
   "source": [
    "#### Tool 1: embed YouTube-transcript or PDF\n",
    "\n",
    "We add chunks of YouTube-transcripts or PDF documents in our vector database for later retrieval by providing a file path for a PDF or a link to a Youtube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328864e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(parse_docstring=True)\n",
    "def embed_document(source: str, doc_type: Literal[\"youtube\", \"pdf\"]) -> str:\n",
    "    \"\"\"Embed (or update) a YouTube video or PDF into the vector store.\n",
    "\n",
    "    Uses deterministic per-chunk IDs derived from a document key so re-embedding\n",
    "    replaces the prior version cleanly. Applies an atomic metadata update to\n",
    "    avoid lost updates when multiple embeddings run concurrently.\n",
    "\n",
    "    Args:\n",
    "        source: For PDFs a local file path. For YouTube a full video URL.\n",
    "        doc_type: The type of document to embed. Must be either \"youtube\" or \"pdf\".\n",
    "\n",
    "    Returns:\n",
    "        A human-readable summary including document key and number of chunks embedded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build documents & add to vector store first (idempotent aside from deletes)\n",
    "        metadata_snapshot = load_metadata(METADATA_FILE)\n",
    "        doc_info = get_documents(source, doc_type)\n",
    "        doc_info, chunks = embed_documents(doc_info, text_splitter, vector_store, metadata_snapshot, doc_type)\n",
    "        (_, title, uploader, doc_key) = doc_info\n",
    "\n",
    "        from chat_with_X_utils.metadata_mangement import atomic_update_metadata\n",
    "\n",
    "        def _apply(current_meta):\n",
    "            current_meta = dict(current_meta)\n",
    "            current_meta[doc_key] = {\n",
    "                'title': title,\n",
    "                'type': doc_type,\n",
    "                'source': source,\n",
    "                'embedded_at': datetime.now().isoformat(),\n",
    "                'num_chunks': len(chunks)\n",
    "            }\n",
    "            if doc_type == 'youtube':\n",
    "                current_meta[doc_key]['uploader'] = uploader\n",
    "            return current_meta, current_meta[doc_key]\n",
    "\n",
    "        atomic_update_metadata(METADATA_FILE, _apply)\n",
    "        return f\"Embedded/Updated {doc_type}: '{title}' ({len(chunks)} chunks) key={doc_key}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error embedding document: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e532d4",
   "metadata": {},
   "source": [
    "#### Tool 2: list documents available in database\n",
    "\n",
    "Here we list which documents are currently available from our vector database by retrieving the information from our metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8367ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(parse_docstring=True)\n",
    "def list_documents() -> str:\n",
    "    \"\"\"List stored documents (sorted by title) with basic stats.\n",
    "\n",
    "    Returns:\n",
    "        Multi-line string enumerating each document, its key, chunk count and date.\n",
    "    \"\"\"\n",
    "    metadata = load_metadata(METADATA_FILE)\n",
    "    if not metadata:\n",
    "        return \"No documents in the database.\"\n",
    "    return get_database_info(metadata) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34303a3",
   "metadata": {},
   "source": [
    "#### Tool 3: select which available document we want to chat with. \n",
    "\n",
    "For this we need to alter the graphs state from within the tool using the [Command](https://langchain-ai.github.io/langgraph/concepts/low_level/#command) object. \n",
    "\n",
    "\n",
    "- Because of the Message API we also need to append a ToolMessage with the correct ToolCallID to the `messages` key in the state. \n",
    "\n",
    "- For this we need to inject the `ToolCallId` tool call using [`InjectedToolCallId`](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html). Note that this usually gets handled automatically when returning a regular String/ToolMessage from the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def select_documents_for_chat(\n",
    "    doc_keys: List[str],\n",
    "    # TODO: Inject the tool call ID so you can attach a ToolMessage with the right ID\n",
    "    tool_call_id: ... = \"\",  \n",
    "):\n",
    "    \"\"\"Select documents (by key) to constrain subsequent retrieval.\n",
    "\n",
    "    Args:\n",
    "        doc_keys: List of document keys to select.\n",
    "        tool_call_id: (Injected) Tool call ID used to attach a ToolMessage.\n",
    "\n",
    "    Returns:\n",
    "        Command updating state.documents_to_chat plus a summary ToolMessage.\n",
    "    \"\"\"\n",
    "    metadata = load_metadata(METADATA_FILE)\n",
    "    summary, valid_documents = create_selection_summary(metadata, doc_keys)\n",
    "    # TODO: Return a Command that updates documents_to_chat and appends a ToolMessage with the summary and correct tool_call_id\n",
    "    return Command(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda27785",
   "metadata": {},
   "source": [
    "#### Tool 4: retrieve embedded document chunks\n",
    "\n",
    "- We want to limit the retrieval to specific documents, so we need to pass the corresponding value of the `documents_to_chat_with` key directly into the tool call using [`InjectedState`](https://langchain-ai.github.io/langgraph/reference/agents/?h=injectedstate#langgraph.prebuilt.tool_node.InjectedState).\n",
    "\n",
    "- This bypasses the possibility for the LLM-agent to make an error here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def retrieve_chunks(\n",
    "    query: str,\n",
    "    # TODO: Inject the select documents (documents_to_chat) from the state directly.\n",
    "    selected_documents: ... = [],\n",
    "    k: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"Retrieve top-k chunks relevant to query over selected documents.\n",
    "\n",
    "    Args:\n",
    "        query: Natural language query to search for.\n",
    "        selected_documents: (Injected) List of doc_keys currently selected; empty means search all.\n",
    "        k: Number of chunks to return (default 5).\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with retrieved chunk excerpts and source citations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filter_dict = {\"doc_key\": {\"$in\": selected_documents}} if selected_documents else None\n",
    "        results = vector_store.similarity_search(query=query, k=k, filter=filter_dict)\n",
    "    \n",
    "        if not results:\n",
    "            return \"No relevant chunks found.\"\n",
    "    \n",
    "        return parse_retrieval(results) \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving chunks: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499c12a",
   "metadata": {},
   "source": [
    "#### Tool 5: delete documents\n",
    "\n",
    "Let's now combine everything we've learned about handling the graph state using injections and the Command object, by writing a tool to\n",
    "\n",
    "- delete documents from our vector store and metadata file\n",
    "\n",
    "- while automatically deselecting them from the `documents_to_chat` key in our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc32a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def delete_documents(\n",
    "    doc_keys: List[str],\n",
    "    # TODO: Inject the current selection (documents_to_chat) from the state directly.\n",
    "    current_selection: ... = [],\n",
    "    # TODO: Inject the tool call ID so you can attach a ToolMessage with the right ID\n",
    "    tool_call_id: ... = \"\",\n",
    "):\n",
    "    \"\"\"Delete one or more documents and prune them from current selection atomically.\n",
    "\n",
    "    Args:\n",
    "        doc_keys: List of document keys to delete (supports multiple at once).\n",
    "        current_selection: (Injected) Current selected doc keys from state.documents_to_chat.\n",
    "        tool_call_id: (Injected) Tool call ID used to attach a ToolMessage.\n",
    "\n",
    "    Returns:\n",
    "        Command that updates documents_to_chat and appends a deletion summary ToolMessage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from chat_with_X_utils.metadata_mangement import atomic_update_metadata, load_metadata\n",
    "        from chat_with_X_utils.tool_utils import delete_documents_from_store, create_deletion_summary\n",
    "\n",
    "        # Perform metadata mutation under atomic lock so parallel deletions don't race.\n",
    "        def _apply(current_meta):\n",
    "            # delete_documents_from_store mutates metadata in-place and returns summary info\n",
    "            info = delete_documents_from_store(current_meta, doc_keys, vector_store)\n",
    "            return current_meta, info  # updated dict + info tuple\n",
    "\n",
    "        info = atomic_update_metadata(METADATA_FILE, _apply)\n",
    "        # info = (not_found, deleted_keys, deleted_title_pairs)\n",
    "        summary, updated_selection = create_deletion_summary(info, current_selection)\n",
    "\n",
    "        # TODO: Return a Command that updates documents_to_chat and appends a ToolMessage with the summary and correct tool_call_id\n",
    "        return Command(...)\n",
    "    except Exception as e:\n",
    "        return f\"Error deleting document(s): {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1542207",
   "metadata": {},
   "source": [
    "### Nodes: Chat-LLM + tools\n",
    "\n",
    "- **Before:** we only use one LLM-based node as our assistant. \n",
    "\n",
    "- **Now:** we put the `delete_documents` tool into a separate node $\\rightarrow$ we want to interrupt the graph's execution and wait for user confirmation as a guard-rail against LLM-induced errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for chat node (moved out of the node function)\n",
    "TOOLS_ALL = [\n",
    "    embed_document,\n",
    "    list_documents,\n",
    "    select_documents_for_chat,\n",
    "    retrieve_chunks,\n",
    "    delete_documents,\n",
    "]\n",
    "\n",
    "# TODO: Define SAFE_TOOLS and SENSITIVE_TOOLS appropriately.\n",
    "SAFE_TOOLS = [...]\n",
    "SENSITIVE_TOOLS = [...]\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful RAG assistant. Use tools for document management and retrieval. \"\n",
    "    \"Always call retrieve_chunks(query=..., k=...) before giving a substantive answer that relies on document content. \"\n",
    "    \"Cite timestamps for YouTube (mm:ss) and page numbers for PDFs when possible. \"\n",
    "    \"If information isn't found, say so explicitly.\"\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS_ALL)\n",
    "\n",
    "WELCOME_MESSAGE = (\n",
    "    \"Welcome to the RAG System! I can help you:\\n\\n\"\n",
    "    \"📚 Document Management:\\n\"\n",
    "    \"- Embed (or update) YouTube videos or PDFs\\n\"\n",
    "    \"- List stored documents (shows each document key)\\n\"\n",
    "    \"- Delete documents by key (will ask for confirmation)\\n\\n\"\n",
    "    \"💬 Chat & Retrieval:\\n\"\n",
    "    \"- Ask questions about your selected documents\\n\"\n",
    "    \"- Retrieve supporting chunks with sources (timestamps/pages)\\n\\n\"\n",
    "    \"Just tell me what you'd like to do!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970517f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: RAGState) -> Dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    current_selection = state.get(\"documents_to_chat\", [])\n",
    "\n",
    "    # First turn: emit a welcome AIMessage without invoking the LLM \n",
    "    if not messages:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=WELCOME_MESSAGE)],\n",
    "            \"documents_to_chat\": current_selection\n",
    "        }\n",
    "\n",
    "    # Invoke the already tool-bound model with a prepended system message\n",
    "    response = llm_with_tools.invoke([\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        *messages\n",
    "    ])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "design_graph.add_node(\"chat\", chat_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes containing tool execution logic (referencing pre-defined tool groups)\n",
    "design_graph.add_node(\"safe_tools\", ToolNode(SAFE_TOOLS))\n",
    "design_graph.add_node(\"sensitive_tools\", ToolNode(SENSITIVE_TOOLS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405b985",
   "metadata": {},
   "source": [
    "### Edges\n",
    "- **Before** our routing funciton router either to the \"multiply_tool\" node if a tool call was present or END if there wasn't.\n",
    "\n",
    "- **Now** we want to route based on the safety of a tool. We write a custom [conditional edge](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges), which returns the name of the correct ToolNode to route to or END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(state: RAGState):\n",
    "    nxt = tools_condition(state)\n",
    "    if nxt == END:\n",
    "        return END\n",
    "    ai_msg = state[\"messages\"][-1]\n",
    "    sensitive = {getattr(t, \"name\", getattr(t, \"__name__\", \"\")) for t in (SENSITIVE_TOOLS or [])} or {\"delete_documents\"}\n",
    "    if any(tc.get(\"name\") in sensitive for tc in getattr(ai_msg, \"tool_calls\", [])):\n",
    "        return \"sensitive_tools\"\n",
    "    return \"safe_tools\"\n",
    "\n",
    "design_graph.add_edge(START, \"chat\")\n",
    "# TODO: Add conditional edges from chat to the tool nodes using route_tools\n",
    "design_graph...\n",
    "design_graph.add_edge(\"safe_tools\", \"chat\")\n",
    "design_graph.add_edge(\"sensitive_tools\", \"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72153c9",
   "metadata": {},
   "source": [
    "### Compile and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8febca",
   "metadata": {},
   "source": [
    "- We only want to user sensitive tools with after getting a user confirmation (*human-in-the-loop*). \n",
    "\n",
    "- So, we use the `interrupt_before` argument in the [compile](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.compile) function, that specifies the node before with the interrupt occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "compiled_graph = design_graph.compile(\n",
    "    checkpointer=memory,\n",
    "    # TODO: Make the graph interrupt before calling sensitive tools\n",
    "    ...,\n",
    ")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Mermaid rendering failed, trying ascii art\")\n",
    "    print(compiled_graph.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a04b9b",
   "metadata": {},
   "source": [
    "### Putting user in the graph stream \n",
    "\n",
    "- If the graph's execution is interrupted, it's state will have the `next` parameter set to the name of the node the execution should continue on, once we invoke the graph again.\n",
    "\n",
    "- During the interrupt, we ask for the user's approval for delete calls. If user says\n",
    "    - **yes (y):** we just need to continue the graph's execution\n",
    "    - **no (n):** we still need to append a `ToolMessage` with the correct tool_call_id to the `messages` key to satisfy the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a550b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def stream_graph_updates_with_interrupt(graph, user_input: str, config: dict, _printed: set):\n",
    "    for event in graph.stream({\"messages\": [HumanMessage(content=user_input)]},\n",
    "                              config, \n",
    "                              stream_mode=\"values\"):\n",
    "        _print_messages_from_stream_event(event, _printed)\n",
    "    # After streaming, check for interrupt (pending sensitive tool execution)\n",
    "    snapshot = graph.get_state(config)\n",
    "    # TODO: Make sure the loop conditions gets triggered if the graph is interrupted\n",
    "    while ...:  \n",
    "        # Ask user approval\n",
    "        try:\n",
    "            decision = input(\"Approve deletion tool call? ('y' to continue / or reason to deny): \")\n",
    "        except Exception:\n",
    "            decision = \"y\"\n",
    "        if decision.strip().lower() == \"y\":\n",
    "            # Continue execution (execute the sensitive tool)\n",
    "            result = graph.invoke(None, config)\n",
    "            _print_messages_from_state(result, _printed)\n",
    "        else:\n",
    "            # Deny: send ToolMessage with denial so model can adjust\n",
    "            last_message = snapshot.values[\"messages\"][-1]\n",
    "            tool_call_id = last_message.tool_calls[0][\"id\"]\n",
    "            # TODO: Invoke the graph with a new message in the graph state while satisfying the tool call ID requirement\n",
    "            denial = graph.invoke({\n",
    "                \"messages\": [\n",
    "                    ToolMessage(\n",
    "                        tool_call_id=...\n",
    "                        ,\n",
    "                        content=f\"Deletion denied by user. Reasoning: {decision} Continue without deleting and assist further.\"\n",
    "                    )\n",
    "                ]\n",
    "            }, config)\n",
    "            _print_messages_from_state(denial, _printed)\n",
    "        snapshot = graph.get_state(config)\n",
    "\n",
    "\n",
    "# Configure a new thread/session\n",
    "config = {\"configurable\": {\"thread_id\": \"rag_session_1\"}}\n",
    "\n",
    "# Track printed message ids to avoid duplicates on stream\n",
    "_printed = set()\n",
    "\n",
    "welcome_result = compiled_graph.invoke({\"messages\": [], \"documents_to_chat\": []}, config)\n",
    "_print_messages_from_state(welcome_result, _printed)\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    stream_graph_updates_with_interrupt(compiled_graph, user_input, config, _printed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
